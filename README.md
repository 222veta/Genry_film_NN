# Genry_film_NN
# Отчёт по работе над кодом модели для определения жанра фильма

## Введение
Проект направлен на разработку модели машинного обучения для классификации жанров фильмов на основе их текстовых описаний. Основное внимание уделено обработке текста, созданию словарей и подготовке данных, чтобы обеспечить высокую точность модели.

---

## Отчёт 1: Подготовка данных

### Описание функций

#### Функция tokenize_text

Цель:
Токенизация текстовых данных для преобразования текстов в последовательности токенов, пригодных для анализа.

Параметры:
- p_raw_text: исходная текстовая строка.
- p_stop_words: список стоп-слов.

Описание работы:
1. Разбиение текста на токены с использованием библиотеки nltk.
2. Удаление знаков препинания для снижения шумов в данных.
3. Приведение всех слов к нижнему регистру для унификации.
4. Исключение стоп-слов (например, предлогов и союзов), чтобы сосредоточиться на значимых терминах.
5. Обработка числовых данных и сокращений для улучшения качества токенизации.

Мыслительный процесс:
При разработке функции основное внимание уделялось сокращению объёма неинформативных данных. Например, удаление знаков препинания и стоп-слов улучшает качество обработки текста и ускоряет обучение модели. Библиотека nltk выбрана за её надёжность и богатый функционал для работы с текстом.

Код:
import nltk
import string

def tokenize_text(p_raw_text, p_stop_words):
    tokenized_str = nltk.word_tokenize(p_raw_text)
    tokens = [i.lower() for i in tokenized_str if i not in string.punctuation]
    filtered_tokens = [i for i in tokens if i not in p_stop_words]
    return filtered_tokens

### Токенизация описания фильмов

Процесс:
Токенизация текстов описаний фильмов выполняется с использованием подготовленных функций. 
Используются библиотеки nltk и pandas для обработки и хранения данных.

Мыслительный процесс:
nltk была выбрана из-за её совместимости с другими библиотеками Python и надёжности работы с текстом. Токенизация применяется как для обучающей, так и для тестовой выборок, чтобы модель могла корректно обрабатывать данные на этапе предсказаний.

Код:
from nltk.corpus import stopwords
train_df['description_tokenized'] = train_df['description'].apply(lambda x: tokenize_text(x, stopwords.words('english')))
test_df['description_tokenized'] = test_df['description'].apply(lambda x: tokenize_text(x, stopwords.words('english')))

---

### Создание словаря

#### Описание процесса
1. Инициализация словаря специальными токенами:
   - <PAD>: для дополнения последовательностей.
   - <START>: для обозначения начала последовательности.
   - <UNKNOWN>: для неизвестных слов.
2. Учёт частоты встречаемости слов для оценки их значимости.
3. Присвоение уникального индекса каждому слову для преобразования текста в числовую форму.
4. Сортировка словаря по частоте встречаемости для оптимизации доступа к наиболее значимым словам.

#### Мыслительный процесс
При создании словаря особое внимание уделялось минимизации числа неизвестных слов, которые могут негативно влиять на качество модели. Инициализация специальными токенами позволяет обрабатывать редкие или отсутствующие слова, что делает модель более устойчивой.

#### Код
vocabulary = {}
max_val = 1000000
vocabulary["<PAD>"] = max_val + 2
vocabulary["<START>"] = max_val + 1
vocabulary["<UNKNOWN>"] = max_val

for tokens in train_df.description_tokenized:
    for word in tokens:
        if word not in vocabulary.keys():
            vocabulary[word] = 1
        else:
            vocabulary[word] += 1

vocabulary = {k: v for k, v in sorted(vocabulary.items(), key=lambda item: item[1], reverse=True)}

cnt = 0
for k in vocabulary.keys():
    vocabulary[k] = cnt
    cnt += 1

---

## Используемые технологии

- Язык программирования: Python 3.11
- Библиотеки:
  - nltk: для обработки текста.
  - pandas: для работы с табличными данными.
  - string: для работы с символами и знаками препинания.
- Инструменты:
  - Jupyter Notebook: для написания и тестирования кода.
  - Git: для контроля версий.

---

## Документация
