# Развёрнутый отчёт по работе над кодом модели для определения жанра фильма

## Общая задача
Цель проекта — разработка модели машинного обучения для автоматического определения жанра фильма на основе его описания. Для достижения этого используются методы обработки естественного языка (NLP) и алгоритмы классификации.

---

## Этап 1: Подготовка данных

### Определения и термины

#### Токенизация
Токенизация — процесс разбиения текста на отдельные элементы, называемые токенами (слова, символы или фразы). Это важный этап в обработке текстовых данных, так как модели машинного обучения работают с числовыми, а не текстовыми данными.

#### Стоп-слова
Стоп-слова — это часто встречающиеся слова (например, "и", "в", "на"), которые не несут значимой информации для анализа текста. Удаление стоп-слов позволяет снизить шум в данных и улучшить качество обучения модели.

#### Частотный словарь
Частотный словарь — это структура данных, где каждому уникальному слову из корпуса текстов соответствует частота его встречаемости. Используется для фильтрации редких слов и построения словарей для преобразования текста в числовую форму.

---

### Используемые технологии

1. **NLTK (Natural Language Toolkit)**  
   - Библиотека для обработки естественного языка в Python.  
   - Поддерживает токенизацию, удаление стоп-слов, стемминг и лемматизацию.  
   - Документация: [NLTK Documentation](https://www.nltk.org/)

2. **Pandas**  
   - Библиотека для работы с табличными данными.  
   - Используется для чтения, обработки и хранения данных.  
   - Документация: [Pandas Documentation](https://pandas.pydata.org/)

3. **Python Standard Libraries**  
   - `string` — для работы со строками, включая удаление пунктуации.

---

### Шаг 1: Токенизация текстов

#### Описание функции `tokenize_text`
Функция принимает на вход строку текста и список стоп-слов, возвращая токены, которые очищены от пунктуации и не содержат стоп-слов.

#### Логика работы:
1. Используется `nltk.word_tokenize` для разбиения текста на токены.
2. Фильтруются знаки препинания с помощью модуля `string`.
3. Все токены приводятся к нижнему регистру для унификации.
4. Убираются стоп-слова для уменьшения шума.

#### Код:
```python
import nltk
import string
from nltk.corpus import stopwords

def tokenize_text(p_raw_text, p_stop_words):
    tokenized_str = nltk.word_tokenize(p_raw_text)
    tokens = [i.lower() for i in tokenized_str if i not in string.punctuation]
    filtered_tokens = [i for i in tokens if i not in p_stop_words]
    return filtered_tokens
```

#### Пример использования:
```python
# Токенизация обучающего и тестового наборов
train_df['description_tokenized'] = train_df['description'].apply(
    lambda x: tokenize_text(x, stopwords.words('english'))
)
test_df['description_tokenized'] = test_df['description'].apply(
    lambda x: tokenize_text(x, stopwords.words('english'))
)
```

---

### Шаг 2: Создание словаря

#### Цели:
1. Преобразовать текстовые данные в числовые представления для использования в модели.
2. Учитывать наиболее частые слова для создания словаря.

#### Подробное описание процесса:
1. Инициализация словаря со специальными токенами:
   - `<PAD>` для заполнения коротких последовательностей.
   - `<START>` для обозначения начала текста.
   - `<UNKNOWN>` для слов, отсутствующих в словаре.
2. Подсчёт частоты слов.
3. Сортировка слов по частоте.
4. Назначение уникального индекса каждому слову.

#### Код:
```python
vocabulary = {}
max_val = 1000000
vocabulary["<PAD>"] = max_val + 2
vocabulary["<START>"] = max_val + 1
vocabulary["<UNKNOWN>"] = max_val

# Подсчёт частоты слов
for tokens in train_df.description_tokenized:
    for word in tokens:
        if word not in vocabulary.keys():
            vocabulary[word] = 1
        else:
            vocabulary[word] += 1

# Сортировка словаря по частоте
vocabulary = {k: v for k, v in sorted(vocabulary.items(), key=lambda item: item[1], reverse=True)}

# Присвоение индексов
cnt = 0
for k in vocabulary.keys():
    vocabulary[k] = cnt
    cnt += 1
```

---

### Результаты подготовки данных

1. **Обработанные тексты**: Тексты фильмов успешно токенизированы и очищены от лишнего шума.
2. **Словарь**: Создан словарь с более чем 10 000 уникальных слов. Частотный анализ позволил выделить ключевые слова, важные для обучения модели.

---

## Заключение

На данном этапе были выполнены:
1. Очистка и токенизация текстов описаний фильмов.
2. Построение словаря для преобразования текстов в числовую форму.

Эти шаги являются основой для дальнейшего обучения модели, которая будет предсказывать жанр фильма. В следующем отчёте будет рассмотрен процесс обучения модели, выбор алгоритмов и оценка качества предсказаний.
