## Отчёт 2: Создание и настройка модели

### Описание архитектуры модели

#### Компоненты архитектуры:

1. **Входной слой:**
   - Принимает последовательности индексов слов из предварительно созданного словаря.
   - Эти индексы представляют слова текстов, предварительно обработанные токенизацией и приведённые к одинаковой длине (путём обрезки или дополнения).

2. **Слой Embedding:**
   - Преобразует индексы слов в многомерные плотные вектора (векторы эмбеддингов).
   - Каждый вектор представляет семантические и синтаксические свойства слова.
   - Количество измерений эмбеддингов (например, 128) определяет их "глубину" и способность кодировать сложные зависимости.

3. **Рекуррентный слой LSTM (Long Short-Term Memory):**
   - Позволяет модели "запоминать" важную информацию из последовательностей текста, игнорируя нерелевантные данные.
   - Преимущества LSTM:
     - Удержание долгосрочных зависимостей благодаря ячейкам памяти.
     - Способность работать с текстами различной длины.
   - Конфигурация:
     - `return_sequences=False` означает, что на выходе возвращается только последний временной шаг (подходит для классификации).

4. **Полносвязный слой (Dense):**
   - Преобразует выходы LSTM в вероятности для каждого класса (жанра).
   - Функция активации softmax нормализует значения до вероятностей.

#### Мыслительный процесс:
LSTM был выбран из-за его эффективности в обработке последовательных данных. Добавление слоя Embedding позволяет модели лучше улавливать отношения между словами, а также снижает размерность входных данных, что ускоряет обучение. Базовая архитектура модели выбрана для того, чтобы установить отправную точку для анализа и последующих оптимизаций.

---

### Используемые технологии и библиотеки:

1. **TensorFlow/Keras:**
   - Библиотека машинного обучения с высокой производительностью.
   - Упрощает разработку глубоких нейронных сетей.

2. **NumPy:**
   - Для выполнения операций с массивами данных, например, преобразования входных текстов в числовые массивы.

3. **Pandas:**
   - Используется для загрузки, анализа и предобработки данных, включая разбиение на тренировочные и валидационные наборы.

4. **Matplotlib/Seaborn (опционально):**
   - Для визуализации метрик обучения, таких как точность и потери.

---

### Код реализации:

```python
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense
from tensorflow.keras.optimizers import Adam

# Определение параметров модели
input_dim = len(vocabulary)  # Размер словаря
output_dim = 128  # Размер эмбеддингов
max_sequence_length = 100  # Максимальная длина последовательности
lstm_units = 64  # Количество ячеек памяти в LSTM
num_classes = len(label_classes)  # Количество жанров

# Создание модели
model = Sequential([
    Embedding(input_dim=input_dim, output_dim=output_dim, input_length=max_sequence_length),
    LSTM(lstm_units, return_sequences=False),
    Dense(num_classes, activation='softmax')
])

# Компиляция модели
model.compile(
    optimizer=Adam(),
    loss='categorical_crossentropy',
    metrics=['accuracy']
)
```

---

### Тренировка модели

#### Данные:
- Набор данных состоит из текстов и меток (жанров).
- Тексты предварительно обработаны: удалены стоп-слова, приведены к нижнему регистру, выполнена токенизация.

#### Конфигурация обучения:
- Оптимизатор: **Adam** (адаптивное обновление весов для ускорения обучения).
- Функция потерь: **categorical_crossentropy** (подходит для задач многоклассовой классификации).
- Метрика: **accuracy** (для оценки качества модели).
- Параметры:
  - Эпохи: 10.
  - Размер батча: 32.

#### Код обучения:
```python
history = model.fit(
    x_train, y_train,
    validation_data=(x_val, y_val),
    epochs=10,
    batch_size=32
)
```

---

### Документация и пояснения

1. **Embedding слой:**
   - Документация: [Keras Embedding Layer](https://keras.io/api/layers/core_layers/embedding/)
   - Преобразует дискретные индексы слов в непрерывные векторы фиксированной размерности.

2. **LSTM слой:**
   - Документация: [Keras LSTM Layer](https://keras.io/api/layers/recurrent_layers/lstm/)
   - Используется для извлечения временных зависимостей.

3. **Adam оптимизатор:**
   - Документация: [Keras Adam Optimizer](https://keras.io/api/optimizers/adam/)
   - Автоматически адаптирует скорость обучения для ускорения сходимости.

4. **Функция потерь categorical_crossentropy:**
   - Документация: [Keras Loss Functions](https://keras.io/api/losses/probabilistic_losses/#categoricalcrossentropy-function)
   - Рассчитывает разницу между предсказанным распределением вероятностей и истинным классом.

#### Рекомендации:
- Для повышения производительности рекомендуется исследовать другие архитектуры (например, GRU или Transformer).
- Увеличение данных с использованием аугментации текстов или предобученных эмбеддингов (например, GloVe, Word2Vec) может улучшить результаты.


