**Отчёт 3: Улучшение модели**

### **Улучшения**

#### **Добавление данных**  
Для улучшения обобщающей способности модели была расширена обучающая выборка. Особое внимание уделено включению данных из редких жанров. Это необходимо для повышения представительности выборки и предотвращения смещения модели в сторону популярных жанров.  
Редкие жанры, такие как "документальный" или "экспериментальный", имеют меньшую долю в выборке, что может приводить к снижению точности предсказания для этих категорий. Включение таких данных помогает сбалансировать модель, что особенно важно для задач классификации, где требуется равномерная точность по всем классам.  

#### **Техники увеличения данных**  
Для увеличения разнообразия данных были применены методы перефразирования текстов. Эти техники включают:  
- **Синонимизацию**: замена слов на синонимы.  
- **Перестановка частей предложения**: изменение порядка слов и фраз, сохраняющее смысл.  
- **Обогащение описаний**: добавление дополнительных пояснений и атрибутов.  

Перефразирование помогает модели лучше адаптироваться к различным формам представления данных, повышая её устойчивость к вариативности пользовательских описаний.  

---

### **Используемая архитектура**

#### **Двунаправленный LSTM**  
Внедрение двунаправленной рекуррентной нейронной сети на основе LSTM (Long Short-Term Memory) позволило существенно улучшить обработку текста, особенно длинных последовательностей. Двунаправленная архитектура обрабатывает текст в двух направлениях: слева направо и справа налево, что позволяет учитывать контекст с обеих сторон каждого слова.  

**Зачем нужен Bi-LSTM?**  
Обычные LSTM-сети (однонаправленные) анализируют текст последовательно, лишь в одном направлении. Однако в некоторых задачах важно учитывать не только предшествующие слова, но и слова, идущие после текущего. Это особенно актуально для описаний фильмов, где ключевые слова для жанров часто зависят от взаимного расположения слов. Например, фраза *"романтическая комедия с элементами драмы"* должна быть интерпретирована с учётом слов как слева, так и справа. Bi-LSTM решает эту задачу, улучшая понимание контекста.  

**Реализация**  
Для реализации модели был использован следующий код:  

```python
from tensorflow.keras.layers import Bidirectional, Embedding, LSTM, Dense
from tensorflow.keras.models import Sequential

model = Sequential([
    Embedding(input_dim=len(vocabulary), output_dim=128, input_length=max_sequence_length),
    Bidirectional(LSTM(64, return_sequences=False)),
    Dense(num_classes, activation='softmax')
])

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
```

Основные элементы:  
1. **Слой Embedding**: преобразует индексы слов в плотные векторные представления фиксированной размерности (128).  
2. **Bi-LSTM**: двунаправленный слой LSTM с 64 единицами памяти.  
3. **Полносвязный слой (Dense)**: финальная классификация на основе выходных данных.  

Оптимизация модели проводилась с использованием **Adam**, поскольку он эффективно справляется с выбором скорости обучения. Функция потерь **categorical_crossentropy** идеально подходит для многоклассовой классификации.  

---

### **Результаты после улучшений**

После внесённых улучшений были достигнуты следующие показатели:  
- **Точность на обучающей выборке**: 88%  
- **Точность на валидационной выборке**: 84%  

Эти результаты подтверждают, что модель стала лучше справляться с задачей классификации текстов, особенно для редких жанров. Повышение точности на валидации свидетельствует о лучшей способности модели к генерализации.  

#### **Потенциальные направления для дальнейших улучшений**  
- Расширение данных с использованием техник синтетической генерации текстов (например, с помощью GPT).  
- Внедрение механизмов внимания (Attention) для улучшения обработки ключевых слов.  
- Гиперпараметрическая оптимизация (размер слоёв, скорость обучения, активационные функции).  

Эти шаги позволят повысить точность модели и её применимость к ещё более сложным задачам.
